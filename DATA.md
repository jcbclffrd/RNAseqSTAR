# Data Management Guide

This document explains how to manage the large data files generated by the RNAseqSTAR pipeline.

## 📦 **Data Overview**

The complete pipeline generates approximately **53GB of data**:

| Directory | Size | Contents | Regenerable? |
|-----------|------|----------|--------------|
| `data/` | ~35GB | Raw FASTQ files (SRR5068882) | ✅ Yes - Auto-downloads |
| `GenomeDir/` | ~10GB | STAR genome index | ✅ Yes - Auto-builds |
| `results/` | ~5GB | BAM alignments & logs | ⚠️ Takes time to regenerate |
| `reference/` | ~4GB | Genome FASTA & GTF | ✅ Yes - Auto-downloads |
| **Total** | **~53GB** | | |

## 🚨 **Important: These Folders Are Git-Ignored**

All data directories are in `.gitignore` and **NOT stored on GitHub**:
- ✅ This keeps the repository small and manageable
- ✅ Prevents uploading large files to GitHub
- ⚠️ **You must regenerate or backup data separately**

## 🔄 **Data Lifecycle**

### **First Time Setup (Automatic)**
When you run `./docker_run_pipeline.sh`, the pipeline automatically:

1. **Downloads SRR5068882 data** → `data/` (~35GB, ~15 min)
2. **Downloads mouse genome** → `reference/` (~4GB, ~5 min)
3. **Builds STAR index** → `GenomeDir/` (~10GB, ~10 min)
4. **Runs alignment** → `results/` (~5GB, ~30-90 min)

**Total first run**: ~1-2 hours + download time

### **Subsequent Runs**
The pipeline **skips existing data**:
- ✅ If `data/` has FASTQ → Skip download
- ✅ If `reference/` has genome → Skip download
- ✅ If `GenomeDir/` has index → Skip building
- ⚠️ `results/` are overwritten each run

## 💾 **Backup Strategies**

### **Strategy 1: Keep Everything Local** (Current State)
**Pros**: Fast re-runs, no download time  
**Cons**: Uses 53GB disk space  
**Use when**: Actively developing/testing

### **Strategy 2: Delete After Success**
Delete large regenerable data, keep only results:
```bash
# Keep only results
rm -rf data/ reference/ GenomeDir/

# Results folder only: ~5GB
```
**Pros**: Saves 48GB disk space  
**Cons**: Must regenerate for new runs (~1 hour)

### **Strategy 3: Complete Clean Slate**
Delete everything, regenerate from scratch:
```bash
# Delete entire project
rm -rf RNAseqSTAR/

# Clone and regenerate
git clone https://github.com/jcbclffrd/RNAseqSTAR.git
cd RNAseqSTAR
./docker_build.sh
./docker_run_pipeline.sh
```
**Pros**: Frees all 53GB  
**Cons**: Full regeneration (~1-2 hours)

### **Strategy 4: Backup Results Only**
Archive just the important BAM files:
```bash
# Compress results for storage
cd RNAseqSTAR
tar -czf results_SRR5068882_$(date +%Y%m%d).tar.gz results/

# Move to backup location
mv results_SRR5068882_*.tar.gz ~/Backups/

# Now safe to delete project
rm -rf RNAseqSTAR/
```
**Pros**: Preserves work (~1GB compressed)  
**Cons**: Lose intermediate files

## 📁 **What's in Each Directory**

### **data/** (~35GB)
```
data/
├── SRR5068882.fastq          # 33GB uncompressed FASTQ
└── SRR5068882/               # SRA cache files
    └── SRR5068882.sra        # 3.7GB original SRA format
```
**Purpose**: Raw sequencing data  
**Regenerate**: `prefetch SRR5068882` + `fasterq-dump`  
**Time**: ~15 minutes with good internet

### **reference/** (~4GB)
```
reference/
├── genome.fa                  # 2.8GB mouse genome (GRCm38)
├── genes.gtf                  # 838MB gene annotations (GENCODE vM21)
└── genes.gtf.backup           # 838MB backup with chr names
```
**Purpose**: Reference genome and gene annotations  
**Regenerate**: Auto-downloads from Ensembl/GENCODE  
**Time**: ~5 minutes

### **GenomeDir/** (~10GB)
```
GenomeDir/
├── Genome                     # Packed genome sequence
├── SA                         # Suffix array
├── SAindex                    # Suffix array index
├── chrName.txt               # Chromosome names
├── chrStart.txt              # Chromosome start positions
└── [other STAR index files]
```
**Purpose**: STAR aligner genome index  
**Regenerate**: `STAR --runMode genomeGenerate`  
**Time**: ~10-30 minutes, CPU intensive

### **results/** (~5GB)
```
results/
├── SRR5068882_Aligned.sortedByCoord.out.bam       # 4.5GB aligned reads
├── SRR5068882_Aligned.sortedByCoord.out.bam.bai   # BAM index
├── SRR5068882_Log.final.out                        # Alignment statistics
├── SRR5068882_Log.out                              # Detailed log
└── SRR5068882_Log.progress.out                     # Progress log
```
**Purpose**: Final alignment results  
**Regenerate**: Re-run full STAR alignment  
**Time**: ~30-90 minutes  
**Note**: ⚠️ This is your scientific output - backup before deleting!

## 🎯 **Recommended Workflow**

### **For Active Development:**
1. Keep all data locally (53GB)
2. Fast iteration and testing
3. Delete when done with project phase

### **For Publication/Archival:**
1. Backup compressed `results/` (~1GB compressed)
2. Document exact pipeline version (git commit/tag)
3. Delete local project (can regenerate from git + SRA)

### **For Sharing with Collaborators:**
**Share the BAM file:**
```bash
# Upload just the BAM to cloud storage
results/SRR5068882_Aligned.sortedByCoord.out.bam  # 4.5GB
results/SRR5068882_Aligned.sortedByCoord.out.bam.bai
```

**Don't share**: Raw FASTQ or genome (collaborators can regenerate)

## ⚠️ **Before Deleting the Project**

### **Checklist:**
- [ ] All code changes committed to git
- [ ] All branches pushed to GitHub (`git push --all`)
- [ ] Important results backed up elsewhere
- [ ] Verify git status is clean (`git status`)
- [ ] Note current commit hash for reproducibility

### **Quick Verification:**
```bash
cd RNAseqSTAR

# Check git status
git status
git log --oneline -3

# Verify everything pushed
git push --dry-run

# Backup results if needed
tar -czf ~/Backups/RNAseqSTAR_results_$(date +%Y%m%d).tar.gz results/

# Now safe to delete
cd ..
rm -rf RNAseqSTAR/
```

## 🔄 **Regenerating Data After Deletion**

### **Full Regeneration:**
```bash
# 1. Clone repository
git clone https://github.com/jcbclffrd/RNAseqSTAR.git
cd RNAseqSTAR

# 2. Build Docker image (~5 min)
./docker_build.sh

# 3. Run complete pipeline (~1-2 hours)
./docker_run_pipeline.sh
```

### **Partial Regeneration:**
If you kept some data folders:
```bash
# Clone repo
git clone https://github.com/jcbclffrd/RNAseqSTAR.git
cd RNAseqSTAR

# Copy back saved data (if you kept it)
cp -r ~/Backups/data ./
cp -r ~/Backups/reference ./
cp -r ~/Backups/GenomeDir ./

# Build and run
./docker_build.sh
./docker_run_pipeline.sh
```

## 💰 **Disk Space Optimization**

### **Minimal Space (Code Only)**
- Git repository: **~10MB**
- No data, regenerate everything when needed
- Clone → Build → Run takes ~1-2 hours

### **Quick Restart (Keep Index)**
- Keep `GenomeDir/`: **~10GB**
- Keep `reference/`: **~4GB**  
- **Total**: ~14GB
- Saves ~20 minutes on subsequent runs
- Must re-download FASTQ (~15 min)

### **Fast Development (Keep All)**
- Keep everything: **~53GB**
- Instant reruns
- Only recompute alignments if needed

## 📚 **External Data Storage Options**

### **For Large BAM Files:**
1. **Cloud Storage**: AWS S3, Google Cloud, Azure
2. **Institutional Storage**: University/lab servers
3. **External Drive**: USB/external SSD
4. **Compressed Archives**: `tar.gz` for long-term storage

### **For Raw FASTQ:**
- **Not necessary** - Always available from SRA database
- **SRA ID**: SRR5068882 (permanent public accession)
- Can re-download anytime with `prefetch SRR5068882`

## 🎓 **Best Practices**

### ✅ **DO:**
- Document pipeline version (git tag) with results
- Backup final BAM files before deletion
- Use compression for archived results
- Keep notes on analysis parameters

### ❌ **DON'T:**
- Commit large data files to git
- Store raw FASTQ long-term (regenerate from SRA)
- Delete results before backing up
- Forget to document which version produced results

## 📊 **Data Size Reference**

```
Pipeline Step               Output Size     Time to Regenerate
──────────────────────────────────────────────────────────────
SRA Download               3.7GB           ~5-10 min
FASTQ Conversion          33GB            ~5 min
Reference Download         4GB             ~5 min  
STAR Index Building       10GB            ~10-30 min
STAR Alignment             5GB             ~30-90 min
──────────────────────────────────────────────────────────────
TOTAL                     ~53GB           ~1-2 hours
```

## 🔗 **Related Documentation**

- **README.md**: Pipeline usage and setup
- **.gitignore**: Shows what's excluded from git
- **Dockerfile**: Shows what software is installed
- **scripts/run_full_pipeline.sh**: Full pipeline implementation

---

**Last Updated**: October 2025  
**Pipeline Version**: v1.0+
